{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1 : Importing dependencies**"
      ],
      "metadata": {
        "id": "Mz2nN-gs8eRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swk3TjuF6hwo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHSyzfUg9alc",
        "outputId": "49479012-6efd-4574-a026-f8017e12b714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=799617add8a518d2a919a6264c1878a761e4ec3bf01fcd29983a94a3355aa5f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/a4/72/df07592cea3ae06b5e846f5e52262f8b16748e829ca354b7df\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=44cd315742ebb2234167f5d704a5813bdb0d86a60d89c12746f855e86f193667\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f3/85/b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=b3e956049112b3a565c3a5911d3d8a6353bc7de713f4ddd02e65c6fdf9868682\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/26/e9/df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqDAg1Fh9pFb",
        "outputId": "17b274d4-a314-4f23-e322-30a5f3085692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 2 : Data preprocessing**"
      ],
      "metadata": {
        "id": "xJxiK0QoBK3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading files**\n",
        "\n",
        "import files from personal Google drive"
      ],
      "metadata": {
        "id": "5QW3faZpBODj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dcy3vSeJsuG0",
        "outputId": "eab1c8c4-4327-4473-9496-4df0388b39aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/bert/train.csv\",\n",
        "    header = None,\n",
        "    names = cols,\n",
        "    engine = \"python\",\n",
        "    encoding = \"latin1\"\n",
        ")"
      ],
      "metadata": {
        "id": "cY7e2DN-jvuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis = 1,\n",
        "          inplace = True)"
      ],
      "metadata": {
        "id": "TqUeNhzykk-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XBGRv9Gsk0Ai",
        "outputId": "254f3a90-4bc6-48d3-a900-b827e2e9aeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1509a4af-a91e-4ac2-b343-30e0e44d77f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1509a4af-a91e-4ac2-b343-30e0e44d77f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1509a4af-a91e-4ac2-b343-30e0e44d77f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1509a4af-a91e-4ac2-b343-30e0e44d77f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "abMME9aLk9sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning"
      ],
      "metadata": {
        "id": "BXx5JBOYlBg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "    # decode tweets included in the lxml format -> BeautifulSoup.get_text() : 유니코드 텍스트만 들어있는 문자열 반환\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # remove metions -> find all the @ signs followed by letters or numbers -> replace them into white space\n",
        "    # r is to indicate that i am writing a regex\n",
        "    # + means that they can be repeated as many times as needed\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # remove url links\n",
        "    # ? : s can or cannot be there -> search http and https//letter&numbers./\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # remove everything that is not letters, ., !, ?\n",
        "    # ^ means not\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # get rid of the spaces that are repeated several times\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "zlvo_CPnlDIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply claean to all the tweets\n",
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "metadata": {
        "id": "8BXhgLfuoANf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = data.sentiment.values\n",
        "# make the label of 4 into 1 (labels are made up of 0 and 4)\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "metadata": {
        "id": "H7_56rakoKAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "need to create a BERT layer to have acces to meta data for the tokenizer (like vocab size)"
      ],
      "metadata": {
        "id": "YA6e_nkXoUnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "# get BERT model from the website\n",
        "# trainable = False : won't fine tune the weights of BERT\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable = False)\n",
        "# get the vocab file for BERT tokenizer\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# lower casing the text file\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "# create tokenizer\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "DKc-cJ7Jofhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"My dog loves strawberries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmHTi4WXrBam",
        "outputId": "6eaaf5f7-e6b1-42d7-a908-a32a4b9976ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'dog', 'loves', 'straw', '##berries', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"My dog loves strawberries.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzBUW8sprKQ8",
        "outputId": "b77393f8-0b07-4acc-f05f-6095cb572ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2026, 3899, 7459, 13137, 20968, 1012]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize every sentences\n",
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "metadata": {
        "id": "NxnikElSuJvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_inputs would be list of encoded sentences applied to a cleaned sentence\n",
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "metadata": {
        "id": "hoB9jJJWuTWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Creation**\n",
        "\n",
        "create padded batches (to pad sentences for each batch independently)\n",
        "\n",
        "add the minimum of padding tokens possible\n",
        "\n",
        "for that, we sort sentences by length, apply padded_batches and then shuffle"
      ],
      "metadata": {
        "id": "ux7xTzN8udFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of all sentences, corresponding label and the length of the sentence\n",
        "# to iterate over data_inputs while having accounts at 'i' so that we can have access to the corresponding label (data_labels[i])\n",
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "# shuffle data_with_len\n",
        "# shuffle because in the initial data file, inputs are sorted according to the label (sentiments)\n",
        "random.shuffle(data_with_len)\n",
        "# sort every sentences according to the length\n",
        "# x[2] is len(sent)\n",
        "data_with_len.sort(key = lambda x: x[2])\n",
        "# get rid of the info of the length and keep the sentence and label\n",
        "# do that only if the sentence length is more than 2 (just keep the long sentences)\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "                for sent_lab in data_with_len if sent_lab[2] > 2]"
      ],
      "metadata": {
        "id": "Yd4xjiAHu6wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences don't have the same length -> call generator\n",
        "# different length of inputs but same length of output\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all, output_types = (tf.int32, tf.int32))"
      ],
      "metadata": {
        "id": "v8gegK1QyDxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "# padded_batch : 입력데이터의 크기가 가변일 때 같은 크기로 읽을 수 있도록 변환해주는 함수\n",
        "# https://kyoungseop.tistory.com/entry/tensorflow-dataset-paddedbatch-%ED%95%A8%EC%88%98\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes = ((None, ), ))"
      ],
      "metadata": {
        "id": "2y4vahl5yRJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ceil() : gets the smaller integer that is higher than the number we pass\n",
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE) # len(sorted_all) is the number of inputs\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "# shuffle <- all_batched is sorted from the shortest to longest\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "# create test and train datasets\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.take(NB_BATCHES_TEST)"
      ],
      "metadata": {
        "id": "AvU-FXNfyxDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 3 : Model Building**"
      ],
      "metadata": {
        "id": "b_wXCa-c-3Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim = 128,\n",
        "                 nb_filters = 50,\n",
        "                 # number of hidden units\n",
        "                 FFN_units = 512,\n",
        "                 nb_classes = 2,\n",
        "                 dropout_rate = 0.1,\n",
        "                 training = False,\n",
        "                 name = \"dcnn\"):\n",
        "        super(DCNN, self).__init__(name = name)\n",
        "\n",
        "        # embedding layer\n",
        "        # Embedding : vectorize words to map them into semantic geometric space\n",
        "        self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
        "        # focus on 2 consecutive words\n",
        "        # Conv1D : shift feature detectors in only one dimension, Extract regional features using filters\n",
        "        self.bigram = layers.Conv1D(filters = nb_filters,\n",
        "                                    kernel_size = 2,\n",
        "                                    padding = \"valid\",\n",
        "                                    activation = \"relu\")\n",
        "        self.trigram = layers.Conv1D(filters = nb_filters,\n",
        "                                    kernel_size = 3,\n",
        "                                    padding = \"valid\",\n",
        "                                    activation = \"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters = nb_filters,\n",
        "                                    kernel_size = 4,\n",
        "                                    padding = \"valid\",\n",
        "                                    activation = \"relu\")\n",
        "        # GlobalMaxPooling1D : Choose and return the largest vector of multiple vector information -> 가장 정확한 값\n",
        "        self.pool = layers.GlobalMaxPooling1D()\n",
        "        # 신경망을 만드는 코드\n",
        "        self.dense_1 = layers.Dense(units = FFN_units, activation = \"relu\")\n",
        "        # need dropout layer to prevent overfitting\n",
        "        # default dropout rate = 0.5\n",
        "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            # units : Number of neurons active in that hidden layer\n",
        "            # activation : Which function will fit the calculation result of the weight and bias of the hidden layer and print it?\n",
        "            # 1 unit -> activation signoid (classification between 0 and 1)\n",
        "            self.last_dense = layers.Dense(units = 1, activation = \"sigmoid\")\n",
        "        else:\n",
        "            # nb_classes unit -> activation softmax\n",
        "            self.last_dense = layers.Dense(units = nb_classes, activation = \"softmax\")\n",
        "\n",
        "    # if the training is false -> apply dropout\n",
        "    # while training -> dropout : in order to prevent overfitting\n",
        "    # while pedicting -> no dropout : in order to see all the results        \n",
        "    def call(self, inputs, training):\n",
        "        # embedding layer\n",
        "        x = self.embedding(inputs)\n",
        "        # first set of output from the first se of Convolutional Layer\n",
        "        x_1 = self.bigram(x)\n",
        "        # apply the absolute maximum\n",
        "        # each of the 50 feature detectors of size 2 -> get 1 number which is maximum activation for the particular feature\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "\n",
        "        # concat all the result and apply to the dense layer\n",
        "        # concat : Concatenates the list of tensors values along dimension axis\n",
        "        # x_1, x_2, x_3 shape : (batch_size, nb_filters) ---- concat ----> merged shape : (batch_size, 3 * nb_filters)\n",
        "        # axis = -1 : Concat based on the lowest dimension\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis = -1)\n",
        "        # 신경망 형성성\n",
        "        merged = self.dense_1(merged)\n",
        "        # apply dropout\n",
        "        merged = self.dropout(merged, training)\n",
        "        # call output\n",
        "        output = self.last_dense(merged)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0NomVTuE_CfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 4 : Training**"
      ],
      "metadata": {
        "id": "wXHwK3ERCOdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "metadata": {
        "id": "MVQjaj4aCRfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn = DCNN(vocab_size = VOCAB_SIZE,\n",
        "            emb_dim = EMB_DIM,\n",
        "            nb_filters = NB_FILTERS,\n",
        "            FFN_units = FFN_UNITS,\n",
        "            nb_classes = NB_CLASSES,\n",
        "            dropout_rate = DROPOUT_RATE)"
      ],
      "metadata": {
        "id": "2eTOVzSnCid8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NB_CLASSES == 2:\n",
        "    # 모델을 컴퓨터가 이해할 수 있도록 compile\n",
        "    Dcnn.compile(loss = \"binary_crossentropy\",\n",
        "                 optimizer = \"adam\",\n",
        "                 metrics = [\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                 optimizer = \"adam\",\n",
        "                 metrics = [\"sparse_categorical_crossentropy\"])"
      ],
      "metadata": {
        "id": "949YIwZLErye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn = Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!\")"
      ],
      "metadata": {
        "id": "o8eEJWe_Fgn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved as{}.\" .format(checkpoint_path))"
      ],
      "metadata": {
        "id": "zeOWRkszGgeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**"
      ],
      "metadata": {
        "id": "20v6olDAG2bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs = NB_EPOCHS,\n",
        "         callbacks = [MyCustomCallback()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTQK4yusG31w",
        "outputId": "921018a2-a805-4d0b-cafe-302a71ee43fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  44240/Unknown - 588s 13ms/step - loss: 0.4146 - accuracy: 0.8114Checkpoint saved as/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok.\n",
            "44240/44240 [==============================] - 588s 13ms/step - loss: 0.4146 - accuracy: 0.8114\n",
            "Epoch 2/5\n",
            "44237/44240 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.8358Checkpoint saved as/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok.\n",
            "44240/44240 [==============================] - 500s 11ms/step - loss: 0.3715 - accuracy: 0.8358\n",
            "Epoch 3/5\n",
            "44238/44240 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.8533Checkpoint saved as/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok.\n",
            "44240/44240 [==============================] - 499s 11ms/step - loss: 0.3379 - accuracy: 0.8533\n",
            "Epoch 4/5\n",
            "44238/44240 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8700Checkpoint saved as/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok.\n",
            "44240/44240 [==============================] - 499s 11ms/step - loss: 0.3042 - accuracy: 0.8700\n",
            "Epoch 5/5\n",
            "44237/44240 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.8847Checkpoint saved as/content/drive/MyDrive/Colab Notebooks/bert/ckpt_bert_tok.\n",
            "44240/44240 [==============================] - 494s 11ms/step - loss: 0.2723 - accuracy: 0.8847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94bd30c460>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 5 : Evaluation**"
      ],
      "metadata": {
        "id": "G2WwnXMFHnzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv4-w6PaHxwN",
        "outputId": "e2cc33e8-80e2-447d-e698-03ef95528cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4915/4915 [==============================] - 32s 7ms/step - loss: nan - accuracy: 0.7650\n",
            "[nan, 0.7650305032730103]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acc on train set = 88.5\n",
        "\n",
        "Acc on test set = 7605"
      ],
      "metadata": {
        "id": "-7z7Cmc7H5Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training = False)\n",
        "\n",
        "    sentiment = math.floor(output * 2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Output of the model : {}\\nPredicted sentiment : negative.\" .format(output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Output of the model : {}\\nPredicted sentiment : positive.\" .format(output))"
      ],
      "metadata": {
        "id": "JCPZDNWfIVFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"This movie was pretty interesting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7IW7KhyI952",
        "outputId": "c343cc60-9ebc-497b-f2e5-95268b65fd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the model : [[0.97219807]]\n",
            "Predicted sentiment : positive.\n"
          ]
        }
      ]
    }
  ]
}