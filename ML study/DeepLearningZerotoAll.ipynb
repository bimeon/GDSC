{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepLearningZerotoAll.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMboUmlBuqeIrN8RHitkgb1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHWYniDJ9PjV","executionInfo":{"status":"ok","timestamp":1650815595095,"user_tz":-540,"elapsed":701,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"e98e44ce-4093-4e7d-bbc3-13d74c787688"},"outputs":[{"output_type":"stream","name":"stdout","text":["    0|    2.4520|     0.376| 45.660004\n","   10|    1.1036|  0.003398|  0.206336\n","   20|    1.0128|  -0.02091|  0.001026\n","   30|    1.0065|  -0.02184|  0.000093\n","   40|    1.0059|  -0.02123|  0.000083\n","   50|    1.0057|  -0.02053|  0.000077\n","   60|    1.0055|  -0.01984|  0.000072\n","   70|    1.0053|  -0.01918|  0.000067\n","   80|    1.0051|  -0.01854|  0.000063\n","   90|    1.0050|  -0.01793|  0.000059\n","  100|    1.0048|  -0.01733|  0.000055\n","tf.Tensor(5.00667, shape=(), dtype=float32)\n","tf.Tensor(2.4946702, shape=(), dtype=float32)\n"]}],"source":["import tensorflow as tf\n","tf.compat.v1.enable_eager_execution()\n","\n","x_data = [1, 2, 3, 4, 5]\n","y_data = [1, 2, 3, 4, 5]\n","\n","W = tf.Variable(2.9)\n","b = tf.Variable(0.5)\n","\n","learning_rate = 0.01\n","\n","for i in range(100 + 1):\n","  with tf.GradientTape() as tape:\n","    hypothesis = W * x_data + b\n","    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n","\n","  W_grad, b_grad = tape.gradient(cost, [W, b])\n","  W.assign_sub(learning_rate * W_grad)\n","  b.assign_sub(learning_rate * b_grad)\n","\n","  if (i % 10 == 0):\n","    print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\" .format(i, W.numpy(), b.numpy(), cost))\n","\n","print(W * 5 + b)\n","print(W * 2.5 + b)"]},{"cell_type":"code","source":["import numpy as np\n","\n","X = np.array([1, 2, 3])\n","Y = np.array([1, 2, 3])\n","\n","def cost_func(W, X, Y):\n","  c = 0\n","  for i in range(len(X)):\n","    c += (W * X[i] - Y[i]) ** 2\n","  return c / len(X)\n","\n","for feed_W in np.linspace(-3, 5, num = 15):\n","  curr_cost = cost_func(feed_W, X, Y)\n","  print(\"{:6.3f} | {:10.5f}\" .format(feed_W, curr_cost))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRSSnYlv_jer","executionInfo":{"status":"ok","timestamp":1650559352876,"user_tz":-540,"elapsed":268,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"fab071e0-a0e4-4fd4-8ecf-d01d69784f3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-3.000 |   74.66667\n","-2.429 |   54.85714\n","-1.857 |   38.09524\n","-1.286 |   24.38095\n","-0.714 |   13.71429\n","-0.143 |    6.09524\n"," 0.429 |    1.52381\n"," 1.000 |    0.00000\n"," 1.571 |    1.52381\n"," 2.143 |    6.09524\n"," 2.714 |   13.71429\n"," 3.286 |   24.38095\n"," 3.857 |   38.09524\n"," 4.429 |   54.85714\n"," 5.000 |   74.66667\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","X = np.array([1, 2, 3])\n","Y = np.array([1, 2, 3])\n","\n","def cost_func(W, X, Y):\n","  hypothesis = X * W\n","  return tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","W_values = np.linspace(-3 ,5, num=15)\n","cost_values = []\n","\n","for feed_W in W_values:\n","  curr_cost = cost_func(feed_W, X, Y)\n","  cost_values.append(curr_cost)\n","  print(\"{:6.3f} | {:10.5f}\" .format(feed_W, curr_cost))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rilBTupqqCt","executionInfo":{"status":"ok","timestamp":1650822341254,"user_tz":-540,"elapsed":281,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"3dacdc17-9614-4aef-a391-edeb5d8bd09a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-3.000 |   74.66667\n","-2.429 |   54.85714\n","-1.857 |   38.09524\n","-1.286 |   24.38095\n","-0.714 |   13.71429\n","-0.143 |    6.09524\n"," 0.429 |    1.52381\n"," 1.000 |    0.00000\n"," 1.571 |    1.52381\n"," 2.143 |    6.09524\n"," 2.714 |   13.71429\n"," 3.286 |   24.38095\n"," 3.857 |   38.09524\n"," 4.429 |   54.85714\n"," 5.000 |   74.66667\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.compat.v1.set_random_seed(0)\n","\n","x_data = [1., 2., 3., 4.]\n","y_data = [1., 3., 5., 7.]\n","\n","W = tf.Variable([5.0])\n","\n","for step in range(300):\n","  hypothesis = W * X\n","  cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","  alpha = 0.01\n","  gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n","  descent = W - tf.multiply(alpha, gradient)\n","  W.assign(descent)\n","\n","  if (step % 10 == 0):\n","    print('{:5} | {:10.4f} | {:10.6f}' .format(\n","        step, cost.numpy(), W.numpy()[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJR6hdfzAU0-","executionInfo":{"status":"ok","timestamp":1650826451736,"user_tz":-540,"elapsed":292,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"5d24678c-dbd5-4876-b3bc-d0bc23fa215f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    0 |    74.6667 |   4.813334\n","   10 |    28.7093 |   3.364572\n","   20 |    11.0387 |   2.466224\n","   30 |     4.2444 |   1.909177\n","   40 |     1.6320 |   1.563762\n","   50 |     0.6275 |   1.349578\n","   60 |     0.2413 |   1.216766\n","   70 |     0.0928 |   1.134412\n","   80 |     0.0357 |   1.083346\n","   90 |     0.0137 |   1.051681\n","  100 |     0.0053 |   1.032047\n","  110 |     0.0020 |   1.019871\n","  120 |     0.0008 |   1.012322\n","  130 |     0.0003 |   1.007641\n","  140 |     0.0001 |   1.004738\n","  150 |     0.0000 |   1.002938\n","  160 |     0.0000 |   1.001822\n","  170 |     0.0000 |   1.001130\n","  180 |     0.0000 |   1.000700\n","  190 |     0.0000 |   1.000434\n","  200 |     0.0000 |   1.000269\n","  210 |     0.0000 |   1.000167\n","  220 |     0.0000 |   1.000103\n","  230 |     0.0000 |   1.000064\n","  240 |     0.0000 |   1.000040\n","  250 |     0.0000 |   1.000025\n","  260 |     0.0000 |   1.000015\n","  270 |     0.0000 |   1.000009\n","  280 |     0.0000 |   1.000006\n","  290 |     0.0000 |   1.000004\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# data and Label\n","x1 = [73., 93., 89., 96., 73.]\n","x2 = [80., 88., 91., 98., 66.]\n","x3 = [75., 93., 90., 100., 70.]\n","Y = [152., 185., 180., 196., 142.]\n","\n","# weights\n","w1 = tf.Variable(10.)\n","w2 = tf.Variable(10.)\n","w3 = tf.Variable(10.)\n","b = tf.Variable(10.)\n","\n","learning_rate = 0.000001\n","\n","for i in range(1000 + 1):\n","  # tf.GradientTape() to record the gradient of the cost function\n","  with tf.GradientTape() as tape:\n","    hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n","    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","  \n","  # calculates the gradients of the cost\n","  w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n","\n","  # update w1, w2, w3 and b\n","  w1.assign_sub(learning_rate * w1_grad)\n","  w2.assign_sub(learning_rate * w2_grad)\n","  w3.assign_sub(learning_rate * w3_grad)\n","  b.assign_sub(learning_rate * b_grad)\n","\n","  if i % 50 == 0:\n","    print(\"{:5} | {:12.4f}\" .format(i, cost.numpy()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWnavV0zTgTm","executionInfo":{"status":"ok","timestamp":1650883786409,"user_tz":-540,"elapsed":8318,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"b18824b8-9a0e-4acd-ecfd-6aa50af2ad77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    0 | 5793889.5000\n","   50 |   64291.1562\n","  100 |     715.2903\n","  150 |       9.8461\n","  200 |       2.0152\n","  250 |       1.9252\n","  300 |       1.9210\n","  350 |       1.9177\n","  400 |       1.9145\n","  450 |       1.9114\n","  500 |       1.9081\n","  550 |       1.9050\n","  600 |       1.9018\n","  650 |       1.8986\n","  700 |       1.8955\n","  750 |       1.8923\n","  800 |       1.8892\n","  850 |       1.8861\n","  900 |       1.8829\n","  950 |       1.8798\n"," 1000 |       1.8767\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","data = np.array([\n","# X1, X2, X3, y\n","[ 73., 80., 75., 152. ],\n","[ 93., 88., 93., 185. ],\n","[ 89., 91., 90., 180. ],\n","[ 96., 98., 100., 196. ],\n","[ 73., 66., 70., 142. ]\n","], dtype=np.float32)\n","\n","# slice data\n","X = data[:, :-1]\n","y = data[:, [-1]]\n","\n","W = tf.Variable(tf.random.normal([3, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","\n","learning_rate = 0.000001\n","\n","# hypothesis, prediction function\n","def predict(X):\n","\treturn tf.matmul(X, W) + b\n","\n","n_epochs = 2000\n","for i in range(n_epochs+1):\n","\t# record the gradient of the cost function\n","\twith tf.GradientTape() as tape:\n","\t\tcost = tf.reduce_mean((tf.square(predict(X) - y)))\n","\n","\t# calculates the gradients of the loss\n","\tW_grad, b_grad = tape.gradient(cost, [W, b])\n","\n","\t# updates parameters (W and b)\n","\tW.assign_sub(learning_rate * W_grad)\n","\tb.assign_sub(learning_rate * b_grad)\n","\n","\tif i % 100 == 0:\n","\t\tprint(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rYZOxO9iXQ0M","executionInfo":{"status":"ok","timestamp":1650884562190,"user_tz":-540,"elapsed":3044,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"70c9c3ff-bb1b-4674-9b85-1eca0173386c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    0 | 21504.1816\n","  100 |     9.8129\n","  200 |     7.1290\n","  300 |     7.0915\n","  400 |     7.0545\n","  500 |     7.0177\n","  600 |     6.9811\n","  700 |     6.9447\n","  800 |     6.9085\n","  900 |     6.8725\n"," 1000 |     6.8367\n"," 1100 |     6.8011\n"," 1200 |     6.7657\n"," 1300 |     6.7304\n"," 1400 |     6.6955\n"," 1500 |     6.6606\n"," 1600 |     6.6259\n"," 1700 |     6.5915\n"," 1800 |     6.5572\n"," 1900 |     6.5231\n"," 2000 |     6.4891\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","x_train = np.array([\n","    [1, 2],\n","    [2, 3],\n","    [3, 1],\n","    [4, 3],\n","    [5, 3],\n","    [6, 2]], dtype=np.float32)\n","y_train = np.array([\n","    [0],\n","    [0],\n","    [0],\n","    [1],\n","    [1],\n","    [1]], dtype=np.float32)\n","\n","x_test = np.array([[5, 2]], dtype=np.float32)\n","y_test = np.array([[1]], dtype=np.float32)\n","\n","# tf.data.Dataset 파이프라인을 이용하여 값을 입력\n","# from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로 자료형에서 데이터셋을 만들 수 있음\n","dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n","W = tf.Variable(tf.zeros([2, 1]), name='weight')\n","b = tf.Variable(tf.zeros([1]), name='bias')\n","\n","# 원소의 자료구조 반환\n","dataset.element_spec\n","\n","def logistic_regression(features):\n","    hypothesis = tf.sigmoid(tf.matmul(features, W) + b)\n","    return hypothesis\n","\n","\n","def loss_fn(features, labels):\n","    hypothesis = logistic_regression(features)\n","    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n","    return cost\n","\n","def grad(hypothesis, features, labels):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss_fn(features, labels)\n","    return tape.gradient(loss_value, [W,b])\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","\n","EPOCHS = 3000\n","\n","for step in range(EPOCHS + 1):\n","    for features, labels in iter(dataset):\n","        hypothesis = logistic_regression(features)\n","        grads = grad(hypothesis, features, labels)\n","        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n","        if step % 100 == 0:\n","            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n","            # print(hypothesis)\n","            \n","def accuracy_fn(hypothesis, labels):\n","    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n","    return accuracy\n","\n","test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n","print('Accuracy: {}%'.format(test_acc * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zEe-s_nqfbD","executionInfo":{"status":"ok","timestamp":1651514834972,"user_tz":-540,"elapsed":24997,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"40907d7e-7c78-4a45-e306-5c72b493ad5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter: 0, Loss: 0.6874\n","Iter: 100, Loss: 0.5776\n","Iter: 200, Loss: 0.5349\n","Iter: 300, Loss: 0.5054\n","Iter: 400, Loss: 0.4838\n","Iter: 500, Loss: 0.4671\n","Iter: 600, Loss: 0.4535\n","Iter: 700, Loss: 0.4420\n","Iter: 800, Loss: 0.4319\n","Iter: 900, Loss: 0.4228\n","Iter: 1000, Loss: 0.4144\n","Iter: 1100, Loss: 0.4066\n","Iter: 1200, Loss: 0.3992\n","Iter: 1300, Loss: 0.3922\n","Iter: 1400, Loss: 0.3855\n","Iter: 1500, Loss: 0.3790\n","Iter: 1600, Loss: 0.3727\n","Iter: 1700, Loss: 0.3667\n","Iter: 1800, Loss: 0.3608\n","Iter: 1900, Loss: 0.3551\n","Iter: 2000, Loss: 0.3496\n","Iter: 2100, Loss: 0.3442\n","Iter: 2200, Loss: 0.3389\n","Iter: 2300, Loss: 0.3338\n","Iter: 2400, Loss: 0.3288\n","Iter: 2500, Loss: 0.3239\n","Iter: 2600, Loss: 0.3192\n","Iter: 2700, Loss: 0.3146\n","Iter: 2800, Loss: 0.3100\n","Iter: 2900, Loss: 0.3056\n","Iter: 3000, Loss: 0.3013\n","Accuracy: 100%\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = [[1, 2, 1, 1],\n"," [2, 1, 3, 2],\n"," [3, 1, 3, 4],\n"," [4, 1, 5, 5],\n"," [1, 7, 5, 5],\n"," [1, 2, 5, 6],\n"," [1, 6, 6, 6],\n"," [1, 7, 7, 7]]\n","\n","y_data = [[0, 0, 1],\n"," [0, 0, 1],\n"," [0, 0, 1],\n"," [0, 1, 0],\n"," [0, 1, 0],\n"," [0, 1, 0],\n"," [1, 0, 0],\n"," [1, 0, 0]]\n","# y_data 는 one-hot encoding 으로 표시함 \n","\n","# convert into numpy and float format\n","x_data = np.asarray(x_data, dtype = np.float32)\n","y_data = np.asarray(y_data, dtype = np.float32)\n","\n","#y의 개수 = 클래스 개수 = label개수\n","dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n","W = tf.Variable(tf.random.normal([4,3], name='weight'))\n","b = tf.Variable(tf.random.normal([3]),name = 'bias')\n","variable = [W, b]\n","\n","dataset.element_spec\n","\n","\n","def softmax_fn (features):\n","    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n","    return hypothesis\n","# tf.matmul(X, W) + b -> tensorflow 의 matrix 의 muliply 를 이용하여 logistic classifier 를 구현\n","# softmax 를 tensorflow 에 구현된 tf.nn.softmax 를 통해 구현\n","\n","def loss_fn (features, labels):\n","    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n","    cost = tf.reduce_mean(-tf.reduce_sum(y_data * tf.math.log(hypothesis), axis =1))\n","    return cost\n","# cost function - cross entropy 를 이용하여 cost 값을 구함\n","\n","def grad (hypothesis, features, labels):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss_fn(features, labels)\n","    return tape.gradient(loss_value, [W, b])\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n","# cost 값을 최소화하기 위해서 gradient 를 이용 -> 변수를 최적으로 update\n","   \n","n_epochs = 3000\n","for step in range(n_epochs + 1):\n","    \n","    for features, labels in iter(dataset):\n","        hypothesis = softmax_fn (features)\n","        grads = grad(hypothesis, features, labels)\n","        optimizer.apply_gradients(grads_and_vars = zip(grads, [W, b]))\n","    \n","    if step % 300 == 0:\n","            print(\"iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n","            \n","a = x_data\n","a = softmax_fn (a)\n","print(hypothesis) #softmax 함수를 통과시킨 x_data\n","\n","#argmax 가장큰 값의index를 찾아줌\n","print(tf.argmax(a,1)) #가설을 통한 예측값\n","print(tf.argmax(y_data,1)) #실제 값"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dERyqD2Pw2rB","executionInfo":{"status":"ok","timestamp":1652133972534,"user_tz":-540,"elapsed":23377,"user":{"displayName":"백야","userId":"15205162731827468443"}},"outputId":"908eeb87-b461-41c9-cd85-cb58c1be32dd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0, Loss: 10.3833\n","iter: 300, Loss: 0.7710\n","iter: 600, Loss: 0.6388\n","iter: 900, Loss: 0.5756\n","iter: 1200, Loss: 0.5367\n","iter: 1500, Loss: 0.5087\n","iter: 1800, Loss: 0.4865\n","iter: 2100, Loss: 0.4679\n","iter: 2400, Loss: 0.4518\n","iter: 2700, Loss: 0.4374\n","iter: 3000, Loss: 0.4245\n","tf.Tensor(\n","[[8.4981909e-03 4.4411242e-02 9.4709063e-01]\n"," [2.0155923e-02 2.3929778e-01 7.4054629e-01]\n"," [4.1164001e-03 3.6114264e-01 6.3474101e-01]\n"," [3.8973831e-03 6.3191706e-01 3.6418560e-01]\n"," [5.6354964e-01 3.9157674e-01 4.4873673e-02]\n"," [2.7564248e-01 7.2399861e-01 3.5889694e-04]\n"," [5.9847891e-01 3.9954081e-01 1.9803292e-03]\n"," [7.0178384e-01 2.9789022e-01 3.2605737e-04]], shape=(8, 3), dtype=float32)\n","tf.Tensor([2 2 2 1 0 1 0 0], shape=(8,), dtype=int64)\n","tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"]}]}]}